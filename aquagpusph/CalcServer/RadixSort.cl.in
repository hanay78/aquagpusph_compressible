/*
 *  This file is part of AQUAgpusph, a free CFD program based on SPH.
 *  Copyright (C) 2012  Jose Luis Cercos Pita <jl.cercos@upm.es>
 *
 *  AQUAgpusph is free software: you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation, either version 3 of the License, or
 *  (at your option) any later version.
 *
 *  AQUAgpusph is distributed in the hope that it will be useful,
 *  but WITHOUT ANY WARRANTY; without even the implied warranty of
 *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *  GNU General Public License for more details.
 *
 *  You should have received a copy of the GNU General Public License
 *  along with AQUAgpusph.  If not, see <http://www.gnu.org/licenses/>.
 */

/** @file
 * @brief Radix sort OpenCL methods.
 * (See Aqua::CalcServer::RadixSort for details)
 * @note The header CalcServer/RadixSort.hcl.in is automatically appended.
 */

/** @brief Initializes the padded values and keys (permutations)
 *
 * The permutations are initialized in such a way each point is permuted on
 * itself
 * @param in_vals Unpadded and unsorted values
 * @param vals Padded but unsorted values
 * @param keys Permutations (keys)
 * @param n Unpadded number of values.
 * @param N Padded number of values.
 */
__kernel void
init(const __global usize* restrict in_vals,
     __global usize* restrict vals,
     __global usize* keys,
     usize n,
     usize N)
{
	// find position in global arrays
	const usize i = get_global_id(0);
	if (i >= N)
		return;

	keys[i] = i;
	if (i < n)
		vals[i] = in_vals[i];
	else
		vals[i] = UINT_MAX;
}

/** Perform the local histograms. The histograms are the number of occurrences
 * of each radix. Since we are working in parallel, the number of ocurrences
 * of each radix will be splited in blocks of dimension _ITEMS * _GROUPS:
 *   | it(0)gr(0)ra(0)          | it(1)gr(0)ra(0)          | ... |
 * it(items)gr(0)ra(0)          | | it(0)gr(1)ra(0)          | it(1)gr(1)ra(0)
 * | ... | it(items)gr(1)ra(0)          | | ...                      | ... | ...
 * | ...                          | | it(0)gr(groups)ra(0)     |
 * it(1)gr(groups)ra(0)     | ... | it(items)gr(groups)ra(0)     | |
 * it(0)gr(0)ra(1)          | it(1)gr(0)ra(1)          | ... |
 * it(items)gr(0)ra(1)          | | ...                      | ... | ... | ... |
 *   | it(0)gr(groups)ra(1)     | it(1)gr(groups)ra(1)     | ... |
 * it(items)gr(groups)ra(1)     | | ...                      | ... | ... | ... |
 *   | it(0)gr(groups)ra(radix) | it(1)gr(groups)ra(radix) | ... |
 * it(items)gr(groups)ra(radix) | where it is the thread, gr is the group, and
 * ra is the radix.
 * @param keys Input unsorted keys allocated into the device.
 * @param histograms Computed histograms.
 * @param pass Pass of the radix decomposition.
 * @param loc_histo Histograms local memory to speed up the process.
 * @param n Number of keys.
 */
__kernel void
histogram(const __global usize* keys,
          __global usize* histograms,
          const uint pass,
          __local usize* loc_histo,
          const usize n)
{
	const usize it = get_local_id(0);
	const usize ig = get_global_id(0);
	const usize gr = get_group_id(0);

	const usize groups = get_num_groups(0);
	const usize items = get_local_size(0);

	// Initializate the histograms in each thread of this work group
	for (uint ir = 0; ir < _RADIX; ir++) {
		loc_histo[ir * items + it] = 0;
	}

	barrier(CLK_LOCAL_MEM_FENCE);

	// Set the keys analized by each thread
	usize size = n / groups / items;
	// If the data has not been transposed we must start reading from a
	// different place of ig
	usize start = ig * size;

	usize key, radix, k;
	for (usize j = 0; j < size; j++) {
		k = j + start;
		if (k >= n)
			return;
		key = keys[k];

		// Extract from the key the corresponding radix.
		// "key >> (pass * _BITS)" discards all the previously parsed data
		// and the comparation "& (_RADIX-1)" will return the radix in the
		// range (0 -> _RADIX-1)
		radix = ((key >> (pass * _BITS)) & (_RADIX - 1));

		// increment the local histogram of the radix
		loc_histo[radix * items + it]++;
	}

	barrier(CLK_LOCAL_MEM_FENCE);

	for (uint ir = 0; ir < _RADIX; ir++) {
		histograms[items * (ir * groups + gr) + it] =
		    loc_histo[ir * items + it];
	}

	// barrier(CLK_GLOBAL_MEM_FENCE);
}

/** perform a parallel prefix sum (a scan) on the local histograms (see
 * Blelloch 1990), retrieving the accumulated histogram. Each workitem
 * cares about two memories.
 * See also http://http.developer.nvidia.com/GPUGems3/gpugems3_ch39.html
 * @param histograms Histograms, or number of ocurrences of each radix,
 * divided by blocks, as shown in histogram() method.
 * @param temp Local memory used to speed up the process.
 * @param globsum Total number of keys at each group (output).
 * @note The histogram histograms will be transformed in the accumulated
 * histogram.
 * @remarks This method is called two times:
 *   -# The first time _HISTOSPLIT global sums are computed, as well as
 * _RADIX*_GROUPS*_ITEMS/_HISTOSPLIT accumulated histograms.
 *   -# The second time the previously computed global sums are transformed in a
 * accumulated histogram.
 */
__kernel void
scan(__global usize* histograms,
     __local usize* temp,
     __global usize* globsum)
{
	const usize it = get_local_id(0);
	const usize ig = get_global_id(0);
	const usize n = get_local_size(0) << 1;
	const usize gr = get_group_id(0);
	usize decale = 1;

	temp[(it << 1)] = histograms[(ig << 1)];
	temp[(it << 1) + 1] = histograms[(ig << 1) + 1];

	// parallel prefix sum (algorithm of Blelloch 1990)
	// In each stage (the first line is the input data) following process is
	// done:
	//   -# [h0, h1, h2, h3, ...]
	//   -# [h0, h0+h1, h2, h2+h3, ...]
	//   -# [h0, h0+h1, h2, h0+h1+h2+h3, ...]
	// Therefore at the end we have the global sum of the local histograms
	// stored in the last component (we want to save this value, and discard it
	// for the next stage).
	for (usize d = n >> 1; d > 0; d >>= 1) {
		barrier(CLK_LOCAL_MEM_FENCE);
		if (it < d) {
			usize ai = decale * ((it << 1) + 1) - 1;
			usize bi = decale * ((it << 1) + 2) - 1;
			temp[bi] += temp[ai];
		}
		decale <<= 1;
	}

	if (it == 0) {
		globsum[gr] = temp[n - 1];
		temp[n - 1] = 0;
	}

	// down sweep phase.
	// Now we are combining the previous result in the inverse order. For
	// instance, with 4 histograms, we will perform the following process (the
	// first line is the input data again):
	//   -# [h0, h0+h1, h2, 0]
	//   -# [h0, 0, h2, h0+h1]
	//   -# [0, h0, h0+h1, h0+h1+h2]
	// Which is the desired accumulated histogram.
	for (usize d = 1; d < n; d <<= 1) {
		decale >>= 1;
		barrier(CLK_LOCAL_MEM_FENCE);

		if (it < d) {
			usize ai = decale * ((it << 1) + 1) - 1;
			usize bi = decale * ((it << 1) + 2) - 1;

			usize t = temp[ai];
			temp[ai] = temp[bi];
			temp[bi] += t;
		}
	}

	barrier(CLK_LOCAL_MEM_FENCE);

	// Save the accumulated histogram
	histograms[(ig << 1)] = temp[(it << 1)];
	histograms[(ig << 1) + 1] = temp[(it << 1) + 1];

	// barrier(CLK_GLOBAL_MEM_FENCE);
}

/** Paste the _HISTOSPLIT accumulated global sums into the splited accumulated
 * histogram, to get a global accumulated histogram.
 * @param histograms Accumulated histogram. At the start of the method the
 * first value of each split of the histogram is zero. When the method ends the
 * first value of each split will be the accumulated number of keys in all the
 * previous splits.
 * @param globsum Accumulated global sums (_HISTOSPLIT components).
 */
__kernel void
paste(__global usize* histograms, __global usize* globsum)
{
	const usize ig = get_global_id(0);
	const usize gr = get_group_id(0);

	const usize s = globsum[gr];

	histograms[(ig << 1)] += s;
	histograms[(ig << 1) + 1] += s;

	// barrier(CLK_GLOBAL_MEM_FENCE);
}

/** Perform permutations using the accumulated histogram.
 * @param in_keys Input unsorted keys.
 * @param out_keys Output sorted keys.
 * @param histograms Accumulated histograms.
 * @param pass Pass of the radix decomposition.
 * @param in_permut Input permutations.
 * @param out_permut Output permutations.
 * @param loc_histo Histogram local memory to speed up the process.
 * @param n Number of keys.
 * @warning Radix sort needs several pass, so the output sorted keys of this
 * pass must be the input unsorted keys of the next pass. Don't forgive to swap
 * the OpenCL identifiers (for keys and permutations).
 */
__kernel void
sort(const __global usize* in_keys,
     __global usize* out_keys,
     __global usize* histograms,
     const unsigned int pass,
     __global usize* in_permut,
     __global usize* out_permut,
     __local usize* loc_histo,
     const usize n)
{
	const usize it = get_local_id(0);
	const usize ig = get_global_id(0);
	const usize gr = get_group_id(0);

	const usize groups = get_num_groups(0);
	const usize items = get_local_size(0);

	// Set the keys analized by each thread
	const usize size = n / groups / items;
	const usize start = ig * (n / groups / items);

	// take the accumulated histogram in the cache
	for (uint ir = 0; ir < _RADIX; ir++) {
		loc_histo[ir * items + it] =
		    histograms[items * (ir * groups + gr) + it];
	}
	barrier(CLK_LOCAL_MEM_FENCE);

	usize newpos, key, radix, k;
	for (usize j = 0; j < size; j++) {
		// Get the key to sort
		k = j + start;
		if (k >= n)
			return;
		key = in_keys[k];

		// Extract from the key the corresponding radix.
		// "key >> (pass * _BITS)" discards all the previously parsed data
		// and the comparation "& (_RADIX-1)" will return the radix in the
		// range (0 -> _RADIX-1)
		radix = ((key >> (pass * _BITS)) & (_RADIX - 1));

		// Get the new position of the key from the histogram
		newpos = loc_histo[radix * items + it];

		// And set the new key
		out_keys[newpos] = key;
		out_permut[newpos] = in_permut[k];

		// The position is filled, modify the histogram to point to the
		// next available one
		newpos++;
		loc_histo[radix * items + it] = newpos;
	}
}

/** Compute the inversed permutations, which allows to know the original
 * position of a key from the sorted one.
 * @param perms Direct permutations (from the unsorted position to the sorted
 * one)
 * @param inv_perms Inverse permutations (from the sorted position to the
 * unsorted one)
 * @param n Number of keys.
 */
__kernel void
inversePermutation(const __global usize* perms,
                   __global usize* inv_perms,
                   usize n)
{
	const usize i = get_global_id(0);
	if (i >= n)
		return;

	inv_perms[perms[i]] = i;
}
